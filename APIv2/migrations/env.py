import asyncio
from logging.config import fileConfig
from sqlalchemy import pool
from sqlalchemy.engine import Connection
from sqlalchemy.ext.asyncio import async_engine_from_config
from alembic import context

# Import your models here
from app.data.connection import Base
from app.config import settings
import app.data.models


# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
target_metadata = Base.metadata

def include_object(object, name, type_, reflected, compare_to) -> bool:
    """Include pgvector types in autogenerated migrations"""
    return True

def render_item(type_, obj, autogen_context) -> bool|None:
    """Custom rendering for pgvector types"""
    if hasattr(obj, 'type') and hasattr(obj.type, '__class__'):
        if 'pgvector' in str(obj.type.__class__):
            return None
    return False

# Add pgvector import to migration template
def process_revision_directives(context, revision, directives) -> None:
    """Post-traitement automatique des migrations générées"""
    if getattr(context.config.cmd_opts, 'autogenerate', False):
        script = directives[0]
        if script.upgrade_ops.ops:
            # Add pgvector import
            script.imports.add("import pgvector.sqlalchemy")

            # DEBUG : Voir toutes les opérations générées
            print("DEBUG - Operations detectees:")

            # NOUVEAU : creuser dans ModifyTableOps
            all_ops = []
            for i, op in enumerate(script.upgrade_ops.ops):
                print(f"  [{i}] Type: {type(op).__name__}")

                # Si c'est ModifyTableOps, récupérer les sous-opérations
                if hasattr(op, 'ops'):
                    print(f"      Sous-operations: {len(op.ops)}")
                    for j, sub_op in enumerate(op.ops):
                        print(f"        [{j}] Type: {type(sub_op).__name__}")
                        if hasattr(sub_op, 'column'):
                            print(f"            Colonne: {sub_op.column.name}")
                            print(f"            Type colonne: {type(sub_op.column.type).__name__}")
                            all_ops.append(sub_op)
                elif hasattr(op, 'column'):
                    print(f"      Colonne: {op.column.name}")
                    print(f"      Type colonne: {type(op.column.type).__name__}")
                    all_ops.append(op)

            # NOUVEAU : Détecter les enums dans toutes les opérations
            enum_creates = []

            for op in all_ops:
                if hasattr(op, 'column') and hasattr(op.column.type, 'name'):
                    # Si c'est un enum PostgreSQL
                    if hasattr(op.column.type, 'enums'):
                        enum_name = op.column.type.name
                        enum_values = tuple(op.column.type.enums)
                        create_sql = f"""DO $$
                          BEGIN
                              IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = '{enum_name}') THEN
                                  CREATE TYPE {enum_name} AS ENUM {enum_values};
                              END IF;
                          END $$;"""
                        print(f"ENUM DETECTE: {create_sql}")

                        # Ajouter la création d'enum au début
                        from alembic.operations.ops import ExecuteSQLOp
                        enum_creates.append(ExecuteSQLOp(create_sql))

            # Injecter les CREATE TYPE au début de upgrade_ops
            if enum_creates:
                print(f"Ajout de {len(enum_creates)} CREATE TYPE")
                script.upgrade_ops.ops = enum_creates + script.upgrade_ops.ops
            else:
                print("Aucun enum detecte")


# Set the database URL from our settings
config.set_main_option("sqlalchemy.url", settings.database_url or "")


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def do_run_migrations(connection: Connection) -> None:
    context.configure(
        connection=connection,
        target_metadata=target_metadata,
        include_object=include_object,
        process_revision_directives=process_revision_directives
    )

    with context.begin_transaction():
        context.run_migrations()


async def run_async_migrations() -> None:
    """In this scenario we need to create an Engine
    and associate a connection with the context.

    """

    connectable = async_engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)

    await connectable.dispose()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode."""

    asyncio.run(run_async_migrations())


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()